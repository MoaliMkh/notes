{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "### The concept of adversarial search and an introduction to Game Theory\n",
    "In previous lectures, we discussed situations in which we had only a single agent. We didn't consider other parameters affecting our environment but in this chapter, a new type of search is introduced which is called **Adversarial Search**. In adversarial search, we define our problem in a multi-agent context. For instance, while playing a game, our agent has to consider the other agent's moves (adversarial moves) to play in an efficient way. Even in some games we can define a winning stategy which means we can guarantee that in every state of our game, no matter how bad it is, our agent is able to win the game.\n",
    "To gather more information about the concept of adversarial search, visit <a href=\"https://www.techslang.com/definition/what-is-adversarial-search/\">this link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Theory Explanation\n",
    "Briefly, Game Theory is designing the strategies and the steps of our player to interact in the best way, according to the rival's steps and strategies. In other words, Game theory is the study of mathematical models of strategic interactions among rational decision-makers. To know the game theory better, you can stop by <a href=\"https://en.wikipedia.org/wiki/Game_theory\">here</a>.\n",
    "To achieve this goal, we need to express our game and its rules and actions in a mathematical form. The common model used to represent games is the model below:\n",
    "<ul>\n",
    "    <li>States of the game: $S_i$, starting with $S_0$</li>\n",
    "Which means how the game will look after each action of competitors.\n",
    "    <li>Players: P = {1,2,...,n}</li>\n",
    "represents number of agents playing the game which varies from 1 to n.\n",
    "    <li>Actions: $A_i$</li>\n",
    "In every state, each player has a set of legitimate moves to change that state to another state by playing its turn.\n",
    "    <li>Transition functions: S x A -> S</li>\n",
    "Transition function is a function which takes a state S, and does the action A to that state, and returns another state which is built as a consequence of that action.\n",
    "    <li>Terminal tests: S -> {True, False}</li>\n",
    "It is a boolean function which determines whether our game has reached a final state or not.\n",
    "    <li>Terminal utilities: S ✕ P -> R </li>\n",
    "It takes a state S, and a player P, and returns a Real number which indicates the score of the player P (The utility of the P, to be more precise) till that moment.\n",
    "    <li> Policy/Strategy: S -> A </li>\n",
    "The Policy is the strategy defined by our AI program. It takes a state S, and according to the program, does action A which is the best action for that situation. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Limits\n",
    "Although the minimax algorithm would be proper for problems with relatively small state space, it isn't an efficient and feasible one for problems with more complicated and larger state space. Since the number of game states it has to examine is exponential in the depth of the tree.\n",
    "\n",
    "Consider a _reasonable_ chess game with $b \\approx 35$ and $m \\approx 100$ . Due to the time complexity of this algorithm which is $O(b^m)$, solving the game is not possible at all. We'll discuss some ideas to solve the problem in details below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-limited search\n",
    "One idea might be running the algorithm up to a specified depth instead of the searching the whole tree which we call _depth-limited search_ .\n",
    "\n",
    "![depth-limited](./images/depth-limited.jpg)\n",
    "\n",
    "But using this technique is not much satisfying because it leads to antoher issue: _How can we find the minimax value while there is no solution at a limited depth?_\n",
    "Recall how we find the minimax value of each node in the original form. We continue searching until we reach a final state and then use recurision to calculate the parents' values. But in the _limited_ format, there is no goal states at depth $k < m$.\n",
    "\n",
    "Now to deal with this issue, we'll introduce _*Evaluation function*_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "The goal of an evaluation function is to answer to the question: _How good is the current position?_ Or _How probable is it to reach to winning terminal state from this position?_\n",
    "\n",
    "Let us make this idea more concrete. An evaluation function returns an _estimate_ of the utility in each game state, just like the _heuristic_ functions that estimate the remaining cost to a goal state which were discussed before.\n",
    "\n",
    "Obviously, defining an appropriate and precise evaluation function is strongly effective on the player's performance. Actually an inaccurate evaluation function may lead to a lost position in the game.\n",
    "\n",
    "Most evaluation functions calculate the value of the position by using a _weighted sum of features_:\n",
    "\n",
    "$Eval(s) = w_1f_1(s) + w_2f_2(s) + ... + w_nf_n(s)$\n",
    "\n",
    "Each of the $f_i$s is calculating a specific _feature_ of the state _s_.For instance, in chess, we would have features for the number of white pawns, black pawns, white queens, black queens, and etc. To better differentiate the effect of each feature, we multiply them by _weights_.\n",
    "\n",
    "Ideally, evaluation function returns exactly the minimax value of the current state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Deepening Search\n",
    "The accuracy of the evaluation function is critical in shallower states. And the deeper in the tree we search, the less the quality of the evaluation function matters.\n",
    "So a good idea is to maximize the depth of our search as much as possible, considering the limits that we have. In other words, we are looking for an algorithm that can returns an acceptable solution whenever we ask. This kind of algorithms are called _anytime algorithms_. These algorithms are expected to find better and better solutions the longer they keeps running.\n",
    "\n",
    "So simply instead of running the depth-limited-search once, we start running the algorithm with an initial depth limit ($k$). Then after we found s policy, increase the depth limit ($k' > k$) and run the algorithm again with this new limit to find a better and more accurate solution. We continue the iteration until the time limit reaches.\n",
    "This alogrithm is called _iterative-deepening search_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Pruning\n",
    "\n",
    "If we take a look at how minimax works, it is noticeable that it explores some parts of the tree that are not necessary at all. In other words, exploring some branches are totally useless but expensive. If we can predict which branches do not worth exploring and _prune_, we can improve minimax algorithm significantly.\n",
    "\n",
    "To better understand better, consider the following example:\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/1.png\" alt=\"prunning1\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "We start searching with respect to the assumption that we visit leftmost nodes first. Finally when we reach the terminal node with utility 4, _we can deduce that the value of it's parent node(which is a *Min* node) is definitely less than or equal to 4_:\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/2.png\" alt=\"prunning2\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "At this point, this information doesn't help us a lot. So let's continue searching. When we visit the terminal node with utility 3, we find that the value for the parent is also 3. An important observation here is that we can _predict_ that the value of the root _is more than or equal to 3_. That's because the root is a * Max * node.\n",
    "\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/3.png\" alt=\"prunning3\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "After that, we continue searching, a terminal leaf with utility 2 will be discovered, and just like before, we can immediately notice that the value of it's parent is _less than or equal to 2_.\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/4.png\" alt=\"prunning4\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "Now, let's ask ourselves: _Is it necessary that we explore the rightmost node?_. The answer is * No *. Because we already know that the value of the root is at least 3, while the maximum value of the right child of root is 2. So we can easily ignore exploring the last node and find that the minimax value of the root is 3:\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/5.png\" alt=\"prunning5\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "This idea helps us finding a general and systematic algorithm to predict the _bad branches_ and stop exploring them, which is called $\\alpha-\\beta $ _pruning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ https://towardsdatascience.com/understanding-the-minimax-algorithm-726582e4f2c6\n",
    "+ Russell, S. J., Norvig, P., &amp; Davis, E. (2022). Artificial Intelligence: A modern approach. Pearson Educación. \n",
    "+ https://slideplayer.com/slide/4380560/\n",
    "+ https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
