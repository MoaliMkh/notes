{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "### The concept of adversarial search and an introduction to Game Theory\n",
    "In previous lectures, we discussed situations in which we had only a single agent. We didn't consider other parameters affecting our environment but in this chapter, a new type of search is introduced which is called **Adversarial Search**. In adversarial search, we define our problem in a multi-agent context. For instance, while playing a game, our agent has to consider the other agent's moves (adversarial moves) to play in an efficient way. Even in some games we can define a winning stategy which means we can guarantee that in every state of our game, no matter how bad it is, our agent is able to win the game.\n",
    "To gather more information about the concept of adversarial search, visit <a href=\"https://www.techslang.com/definition/what-is-adversarial-search/\">this link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Theory Explanation and Deterministic Games\n",
    "Briefly, Game Theory is designing the strategies and the steps of our player to interact in the best way, according to the rival's steps and strategies. In other words, Game theory is the study of mathematical models of strategic interactions among rational decision-makers. To know the game theory better, you can stop by <a href=\"https://en.wikipedia.org/wiki/Game_theory\">here</a>.\n",
    "To achieve this goal, we need to express our game and its rules and actions in a mathematical form. The common model used to represent games is the model below:\n",
    "<ul>\n",
    "    <li>States of the game: $S_i$, starting with $S_0$</li>\n",
    "Which means how the game will look after each action of competitors.\n",
    "    <li>Players: P = {1,2,...,n}</li>\n",
    "represents number of agents playing the game which varies from 1 to n.\n",
    "    <li>Actions: $A_i$</li>\n",
    "In every state, each player has a set of legitimate moves to change that state to another state by playing its turn.\n",
    "    <li>Transition functions: S x A -> S</li>\n",
    "Transition function is a function which takes a state S, and does the action A to that state, and returns another state which is built as a consequence of that action.\n",
    "    <li>Terminal tests: S -> {True, False}</li>\n",
    "It is a boolean function which determines whether our game has reached a final state or not.\n",
    "    <li>Terminal utilities: S ✕ P -> R </li>\n",
    "It takes a state S, and a player P, and returns a Real number which indicates the score of the player P (The utility of the P, to be more precise) till that moment.\n",
    "    <li> Policy/Strategy: S -> A </li>\n",
    "The Policy is the strategy defined by our AI program. It takes a state S, and according to the program, does action A which is the best action for that situation. \n",
    "</ul>\n",
    "\n",
    "## Different catergories of games\n",
    "Before we discuss types of games, we need to define a concept named **Utility Function** first. Each player has some states that will be more willing in them. For example, if the player P has a State S which is more satisfied in it, this means that P has better moves in S toward the goal state and is able to win the game more easily. So each player has a utility function which indicates that in every state, how satisfied the player is and what is the best result that it can score, starting with that state. Naturally in goal states, which the game has been finished, the utility function is maximum for the player.\n",
    "<ul>\n",
    "    <li>Zero-Sum games</li>\n",
    "    In Zero-Sum games, the sum of utility functions for all the agents is equal to zero. For instance, in a two-agent game, if the player P1 has the utility finction with the value of 10 in an specific state, the player P2 has a utility function with the value -10 in that state. So this game becomes totally adversarial and competetive in a way that we have a single value for utility function and one of the players wants to maximize that value and another one wants to minimize it.\n",
    "    <li>General games</li>\n",
    "    In this type of games, each of the agents has its own utility function. These utility functions may be independant, so in some stages of the game, the agents may even have to cooperate in addition to competing and indifferting to increase their own utility function value. This type of the game is more general and also complicated compared to Zero-Sum games.\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differnt Types of Trees for games\n",
    "## Single Agent Trees\n",
    "Consider the game Pacman which has only an agent playing the game. Starting from an initial state, our agent can either move to left or right. based on this, and its following moves, we can form a tree called **Single Agent Tree**. Each node of this tree indicates an state of the game and the utility function has an specific value in every one of them. We call the leaves of the tree **The Terminal Nodes**. In these terminal nodes, the game is finshed and the utility values are defined. The main question is how can we obtain the utility value of a node? According on what's been said, we have the utility values of the leaves. Recursively, to get the utility value of a node N, we calculate the utility value of every successor of N and we choose the maximum of those numbers as the utility value of the node N.\n",
    "## Adversarial Agent Trees\n",
    "Like the previous Example, we explain this concept with the game Pacman but this time, the game has another agent called the adversarial agent or specific to this game, **The Ghost**.\n",
    "Like Single Agent Trees, we can form a tree to express the moves of the agents, but the differnce is that at each level of the tree, only one agent can play. For example if we start with the player P1 at the root of the tree, based on its move, we gain 2 successors and we go to the first level of the tree. At this level of tree, only the other player called P2 can move and each of the 2 nodes will have 2 successors based on P2's move. So we have 4 nodes in the second level and the game will continue this way till the end. In this Configuration, how can we calculate the utility value of nodes? This will be our next topic.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax Strategy\n",
    "To gain the value of every node in Adversarial tree, we classify the nodes as two groups:\n",
    "<ol>\n",
    "    <li>The nodes our agent does action on them. The utility value of these kind of nodes is assigned by the maximum of the utility values of its successors. Because we want the maximum utility value for our agent. (The max side of the Strategy) </li>\n",
    "    <li>The nodes that the adversial agent does action on them. The utility value of these kind of nodes is assigned by the minimum of the utility values of its successors. Because we want the minimum utility value for the adversary agent. (The min side of the Strategy) </li>\n",
    "</ol>\n",
    "Now that we have every node's utlity value, we are able to find the best action for each state. The main idea is at each state, we choose the node with the greatest minimax value and move to it.\n",
    "Why does this strategy work? Because while forming the tree and assigning the utility function, we've always considered the best move for the enemy in his/her point of view and the worst move in our point of view. So we assumed that the enemy always plays its best shot. Now Two situations are possible to emerge:\n",
    "<ol>\n",
    "    <li>The adversary agent is very smart. In this case, we are ready beacuse the strategy is based on this situation.</li>\n",
    "    <li>The adversary agent is too naive and may do some inoptimal actions. In this case, we may even score more than what we gained as utillty value.</li>\n",
    "</ol>\n",
    "As a conclusion of what's said before, we can say that the utility value that we calculate for nodes in minimax strategy, is a ceiling value of what we may actually score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudocode of Minimax Strategy\n",
    "```python\n",
    "def max_value(state):\n",
    "    if leaf?(state), return U(state)\n",
    "    initialize v = -∞\n",
    "    for each c in children(state):\n",
    "        v = max(v, min_value)\n",
    "    return v\n",
    "\n",
    "\n",
    "def min_value(state):\n",
    "    if leaf?(state), return U(state)\n",
    "    initialize v = ∞\n",
    "    for each c in children(state):\n",
    "        v = min(v, max_value)\n",
    "    return v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Minimax Strategy\n",
    "## Complete:\n",
    "Yes, It is complete if the tree formed for the game is finite.\n",
    "\n",
    "## Optimal:\n",
    "Yes, It is optimal if the opponent acts optimally not naivly. otherwise, we will surely win the game but not in an optimal way because we could've won the game sooner and with a higher score.\n",
    "    \n",
    "## Time Complexity:\n",
    "$O(b^m)$, as it's obvious, it's dependant on the branching factor and the level the tree goes down. So on a game like chess with $b$ = $35$ and $m$ = $100$, we can't use this strategy because the time complexity will be great.\n",
    "\n",
    "## Space Complexity:\n",
    "$O(bm)$, because it's like DFS and only stores the nodes on the current path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Limits\n",
    "Although the minimax algorithm would be proper for problems with relatively small state space, it isn't an efficient and feasible one for problems with more complicated and larger state space. Since the number of game states it has to examine is exponential in the depth of the tree.\n",
    "\n",
    "Consider a _reasonable_ chess game with $b \\approx 35$ and $m \\approx 100$ . Due to the time complexity of this algorithm which is $O(b^m)$, solving the game is not possible at all. We'll discuss some ideas to solve the problem in details below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-limited search\n",
    "One idea might be running the algorithm up to a specified depth instead of the searching the whole tree which we call _depth-limited search_ .\n",
    "\n",
    "![depth-limited](./images/depth-limited.jpg)\n",
    "\n",
    "But using this technique is not much satisfying because it leads to antoher issue: _How can we find the minimax value while there is no solution at a limited depth?_\n",
    "Recall how we find the minimax value of each node in the original form. We continue searching until we reach a final state and then use recurision to calculate the parents' values. But in the _limited_ format, there is no goal states at depth $k < m$.\n",
    "\n",
    "Now to deal with this issue, we'll introduce _*Evaluation function*_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "The goal of an evaluation function is to answer to the question: _How good is the current position?_ Or _How probable is it to reach to winning terminal state from this position?_\n",
    "\n",
    "Let us make this idea more concrete. An evaluation function returns an _estimate_ of the utility in each game state, just like the _heuristic_ functions that estimate the remaining cost to a goal state which were discussed before.\n",
    "\n",
    "Obviously, defining an appropriate and precise evaluation function is strongly effective on the player's performance. Actually an inaccurate evaluation function may lead to a lost position in the game.\n",
    "\n",
    "Most evaluation functions calculate the value of the position by using a _weighted sum of features_:\n",
    "\n",
    "$Eval(s) = w_1f_1(s) + w_2f_2(s) + ... + w_nf_n(s)$\n",
    "\n",
    "Each of the $f_i$s is calculating a specific _feature_ of the state _s_.For instance, in chess, we would have features for the number of white pawns, black pawns, white queens, black queens, and etc. To better differentiate the effect of each feature, we multiply them by _weights_.\n",
    "\n",
    "Ideally, evaluation function returns exactly the minimax value of the current state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Deepening Search\n",
    "The accuracy of the evaluation function is critical in shallower states. And the deeper in the tree we search, the less the quality of the evaluation function matters.\n",
    "So a good idea is to maximize the depth of our search as much as possible, considering the limits that we have. In other words, we are looking for an algorithm that can returns an acceptable solution whenever we ask. This kind of algorithms are called _anytime algorithms_. These algorithms are expected to find better and better solutions the longer they keeps running.\n",
    "\n",
    "So simply instead of running the depth-limited-search once, we start running the algorithm with an initial depth limit ($k$). Then after we found s policy, increase the depth limit ($k' > k$) and run the algorithm again with this new limit to find a better and more accurate solution. We continue the iteration until the time limit reaches.\n",
    "This alogrithm is called _iterative-deepening search_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Pruning\n",
    "\n",
    "If we take a look at how minimax works, it is noticeable that it explores some parts of the tree that are not necessary at all. In other words, exploring some branches are totally useless but expensive. If we can predict which branches do not worth exploring and _prune_, we can improve minimax algorithm significantly.\n",
    "\n",
    "To better understand better, consider the following example:\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/1.png\" alt=\"prunning1\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "We start searching with respect to the assumption that we visit leftmost nodes first. Finally when we reach the terminal node with utility 4, _we can deduce that the value of it's parent node(which is a *Min* node) is definitely less than or equal to 4_:\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/2.png\" alt=\"prunning2\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "At this point, this information doesn't help us a lot. So let's continue searching. When we visit the terminal node with utility 3, we find that the value for the parent is also 3. An important observation here is that we can _predict_ that the value of the root _is more than or equal to 3_. That's because the root is a * Max * node.\n",
    "\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/3.png\" alt=\"prunning3\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "After that, we continue searching, a terminal leaf with utility 2 will be discovered, and just like before, we can immediately notice that the value of it's parent is _less than or equal to 2_.\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/4.png\" alt=\"prunning4\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "Now, let's ask ourselves: _Is it necessary that we explore the rightmost node?_. The answer is * No *. Because we already know that the value of the root is at least 3, while the maximum value of the right child of root is 2. So we can easily ignore exploring the last node and find that the minimax value of the root is 3:\n",
    "\n",
    "<div width=\"50%\" height=\"50%\">\n",
    "    <img src=\"./images/5.png\" alt=\"prunning5\" width=\"50%\" height=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "This idea helps us finding a general and systematic algorithm to predict the _bad branches_ and stop exploring them, which is called $\\alpha-\\beta $ _pruning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other types of games:\n",
    "On top of what's been discussed before, we can count another type of game called **Mixed Layer Types**. In this type, in addition to players' action, we have another random factor affecting the game. So in our computations we must consider this factor. Foe example, Backgammon is a Mixed layer types game, because rolling the dice will affect the states and result of the game.\n",
    "\n",
    "To do some calculations about the depth of the game tree, we can state:\n",
    "\n",
    "$b$ = $21$ because 21 dintinct states are created as an outcome of rolling 2 dices.\n",
    "\n",
    "$legal moves \\approx 20$ \n",
    "\n",
    "So if we want to build the tree till the depth of 2, we will have $20 * (21 * 20) ^ 3$ leaves and termainal states. as we go deeper in tree, this number increases.\n",
    "\n",
    "As the deepness increases, the probability to reach a goal state gets smaller and limiting the depth gets less damaging. So the usefulness of search will be threatened and prunning becomes trickier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Utilities\n",
    "In games with multiple players, terminal states have utility n-tuples values and each player tries to maximize its own part. This can make cooperation strategies viable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Adverserial Search is used when there are other agents in play, probably with conflicting objectives as in most games. Here, we are interested in finding a **Strategy of Playing** that determines which action we should take in different possible scenarios.\n",
    "\n",
    "Games can be classified into two main categories based on utility functions:\n",
    "- Zero-sum Games: agents have opposite utilities (pure competition)\n",
    "- General Games:  independent utilities (cooperation may be beneficial)\n",
    "\n",
    "We can represent possible game states using Game Trees with the initial state as the root node. Each node is followed by its children, representing different actions. Therefore terminal states are the leaf nodes.\n",
    "\n",
    "Optimal strategy against an opponent playing perfectly can be found using **Minimax** algorithm. We can also determine a minimum payoff that can be achieved in every game.\n",
    "A complete search is often infeasible in minimax due to resource limits. So we can:\n",
    "- Replace the utility function with an **Evaluation Function** that estimates utility.\n",
    "- Do **Minimax Pruning**: While calculating minimax values, it is sometimes possible to detect and \"prune\" some suboptimal branches.\n",
    "\n",
    "Uncertain factors (like rolling dice) are represented by Expectation Nodes in a game tree. Optimal strategy in such games can be found using **Expectimax Search**.\n",
    "\n",
    "**Final Note**: While we are looking for a good strategy of playing, Optimism and Pessimism can both result in suboptimal behaviour so it's important to evaluate each problem realistically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ https://towardsdatascience.com/understanding-the-minimax-algorithm-726582e4f2c6\n",
    "+ Russell, S. J., Norvig, P., &amp; Davis, E. (2022). Artificial Intelligence: A modern approach. Pearson Educación. \n",
    "+ https://slideplayer.com/slide/4380560/\n",
    "+ https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf0f1093e9a05e91d616174468df6ede651877b2c960cbb124380d685ad06d65"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
